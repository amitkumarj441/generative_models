## Experimental Scripts and Kernels for LLMs in general


### Commercially-viable LLMs

| Model                                                        | Arch                                                         | License      | Params              | Seq Len | FP Format     | VRAM Infer                                       | Lib                                                          | Tokenizer               | Comments                                                     | Other flavours                               |
|--------------------------------------------------------------|--------------------------------------------------------------|--------------|---------------------|---------|---------------|--------------------------------------------------|--------------------------------------------------------------|-------------------------|--------------------------------------------------------------|----------------------------------------------|
| [bigcode/starcoder](https://huggingface.co/bigcode/starcoder) | GPT                                                          | OpenRAIL-Mv1 | 15B                 | 8k      | fp32          | 60Gb<br>~30Gb fp/bf16<br>~16Gb 8bit<br>~8Gb 4bit | Megatron-LM [fork](https://github.com/bigcode-project/Megatron-LM) | GPT2Tokenizer 49152     | FlashAttn, MQA, FIM, 1T=250Bx4 tokens of StarCoderData       | starcoderplus, starchat-beta (humanEval ðŸ“‰)   |
| [Salesforce/codegen2](https://github.com/salesforce/CodeGen2) | GPT ([J](https://gist.github.com/moyix/7896575befbe1b99162ccfec8d135566)) | Apache 2.0   | 1B, 3.7B<br>7B, 16B | 1k      | 7b fp32 28Gb  |                                                  | [JaxFormer](https://github.com/salesforce/jaxformer)         | GPT2Tokenizer 51200     | RoPE, FIM, The Stack dedup v1.1,                             | -instruct research-only                      |
| [Salesforce/codegen2.5](https://github.com/salesforce/CodeGen/tree/main/codegen25) | LLaMA                                                        | Apache 2.0   | 7B                  | 2k      | fp32 28Gb     |                                                  | JaxFormer                                                    | Tiktoken 51200          | FlashAttn, Triton, FIM, 1.4T=300Bx4+ tokens of StarCoderData | -mono Python, -instruct research-only        |
| [Salesforce/xgen-7b-8k-base](https://huggingface.co/Salesforce/xgen-7b-8k-base) | LLaMA                                                        | Apache 2.0   | 7B                  | 8k      |               |                                                  |                                                              | Tiktoken                |                                                              | -inst research only                          |
| [OpenLLaMA v2](https://github.com/openlm-research/open_llama) | LLaMA                                                        | Apache 2.0   | 3B,7B<br>13B        | 2k      | 7b fp16 14Gb  |                                                  | PyTorch/HF, <br>JAX/EasyLM                                   | HF (fast) tokenizer 32k | 1T tokens of RedPajama + StarCoderData + Falcon              |                                              |
| [replit-code-v1-3b](https://github.com/replit/ReplitLM/tree/main/replit-code-v1-3b) | MPT                                                          | CC BY-SA 4.0 | 2.7B                | 2k      | fp32 10Gb     |                                                  | Mosiac [LLM Foundry](https://github.com/mosaicml/llm-foundry) | SentencePiece 32768     | FlashAttn, Triton, AliBi, FasterTransformer, The Stack dedup v1.2 |                                              |
| [MPT](https://huggingface.co/mosaicml/mpt-7b)                | MPT                                                          | Apache 2.0   | 7B, 30B             | 8k      |               |                                                  | LLM Foundry                                                  |                         | FlashAttn, Titon, ALiBi, FT<br>1T tokens                     | -instruct CC-By-SA-3.0 -chat CC-By-NC-SA-4.0 |
| [Falcon](https://huggingface.co/blog/falcon)                 | GPT (RW)                                                     | Apache 2.0   | 7B, 40B             | 2k      | bf16          | 7B ~15Gb<br>40B ~90Gb                            |                                                              | TokenizerFast 65024     | 1T RefinedWeb, GPTQ                                          | -instruct Apach 2.0 (!)                      |
| [StableLM](https://github.com/Stability-AI/StableLM)         | GPT (NeoX)                                                   | CC-By-SA-4.0 | 3B, 7B              | 4k      | fp32 16Gb     | 3b fp16 12Gb<br>7b 24Gb 7b                       | GPT-NeoX                                                     |                         | RoPE, 1.5T tokens of The Pile                                | -tuned CC-By-NC-SA-4.0                       |
| [RedPajama-INCITE-Base-7B-v0.1](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1) | GPT (NeoX)                                                   | Apache 2.0   | 7B                  | 2k      | fp16          | 16Gb                                             |                                                              | 50432                   | RoPE                                                         | base, instruct, chat                         |
